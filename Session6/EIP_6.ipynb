{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP_6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prateekgulati/EIP/blob/master/Session6/EIP_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IP15p0W_i41",
        "colab_type": "text"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANpkIMcpuVY_",
        "colab_type": "code",
        "outputId": "8de5151d-198c-402d-ec76-21036b409bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V70c8zKK_pxJ"
      },
      "source": [
        "### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjf91NgGKIsD",
        "colab_type": "code",
        "outputId": "f487ead6-ca34-4978-a4ac-6aee6475fb46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4ZE8SZj_s7d",
        "colab_type": "text"
      },
      "source": [
        "### Load Data file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGiKC5hzunzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "import requests\n",
        "data = requests.get(\"http://www.gutenberg.org/cache/epub/11/pg11.txt\").text\n",
        "raw_text = data.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrXIzmOG_x6a",
        "colab_type": "text"
      },
      "source": [
        "### Remove Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fXvwGPCjI-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "raw_text = re.sub(r'[^\\w\\s]','',raw_text)\n",
        "raw_text = re.sub(r'\\r\\n','\\n',raw_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwj6qHDD_2zW",
        "colab_type": "text"
      },
      "source": [
        "### Character to Int Mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8aITLePu7JS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRr3e_yROe_w",
        "colab_type": "code",
        "outputId": "fc0ccfd6-1bbc-4c0e-c965-30a809b89b21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  154864\n",
            "Total Vocab:  39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzLRE6cRAABV",
        "colab_type": "text"
      },
      "source": [
        "### Input Data with padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csn0UphfOiDC",
        "colab_type": "code",
        "outputId": "de4f010f-95b9-4d9e-ced9-91925181cf2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "dataX=pad_sequences(dataX)\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  154764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diPraFCiOpGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TX-dVYBAJ5b",
        "colab_type": "text"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlPR9qS4OtzM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "9dd5783b-38a1-44a0-c5ae-ec9d3f666428"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0722 05:05:07.734703 139985236694912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0722 05:05:07.773721 139985236694912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0722 05:05:07.780106 139985236694912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0722 05:05:08.090466 139985236694912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0722 05:05:08.100724 139985236694912 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0722 05:05:08.394034 139985236694912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0722 05:05:08.415783 139985236694912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSPHFCfHAOzS",
        "colab_type": "text"
      },
      "source": [
        "### CheckPoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMCFBh5ZOviV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yBa3LKwAR7-",
        "colab_type": "text"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmJLQz50Ox9F",
        "colab_type": "code",
        "outputId": "d02670c4-82f1-4706-ea1d-b8a151b134b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, y, epochs=100, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0721 13:07:06.441768 140515360925568 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "154764/154764 [==============================] - 332s 2ms/step - loss: 2.7270\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.72704, saving model to weights-improvement-01-2.7270.hdf5\n",
            "Epoch 2/100\n",
            "154764/154764 [==============================] - 329s 2ms/step - loss: 2.3913\n",
            "\n",
            "Epoch 00002: loss improved from 2.72704 to 2.39134, saving model to weights-improvement-02-2.3913.hdf5\n",
            "Epoch 3/100\n",
            "154764/154764 [==============================] - 330s 2ms/step - loss: 2.1535\n",
            "\n",
            "Epoch 00003: loss improved from 2.39134 to 2.15350, saving model to weights-improvement-03-2.1535.hdf5\n",
            "Epoch 4/100\n",
            "154764/154764 [==============================] - 329s 2ms/step - loss: 1.9916\n",
            "\n",
            "Epoch 00004: loss improved from 2.15350 to 1.99157, saving model to weights-improvement-04-1.9916.hdf5\n",
            "Epoch 5/100\n",
            "154764/154764 [==============================] - 327s 2ms/step - loss: 1.8755\n",
            "\n",
            "Epoch 00005: loss improved from 1.99157 to 1.87552, saving model to weights-improvement-05-1.8755.hdf5\n",
            "Epoch 6/100\n",
            "154764/154764 [==============================] - 330s 2ms/step - loss: 1.7874\n",
            "\n",
            "Epoch 00006: loss improved from 1.87552 to 1.78738, saving model to weights-improvement-06-1.7874.hdf5\n",
            "Epoch 7/100\n",
            "154764/154764 [==============================] - 326s 2ms/step - loss: 1.7141\n",
            "\n",
            "Epoch 00007: loss improved from 1.78738 to 1.71406, saving model to weights-improvement-07-1.7141.hdf5\n",
            "Epoch 8/100\n",
            "154764/154764 [==============================] - 325s 2ms/step - loss: 1.6548\n",
            "\n",
            "Epoch 00008: loss improved from 1.71406 to 1.65476, saving model to weights-improvement-08-1.6548.hdf5\n",
            "Epoch 9/100\n",
            "154764/154764 [==============================] - 330s 2ms/step - loss: 1.6012\n",
            "\n",
            "Epoch 00009: loss improved from 1.65476 to 1.60117, saving model to weights-improvement-09-1.6012.hdf5\n",
            "Epoch 10/100\n",
            "154764/154764 [==============================] - 323s 2ms/step - loss: 1.5560\n",
            "\n",
            "Epoch 00010: loss improved from 1.60117 to 1.55604, saving model to weights-improvement-10-1.5560.hdf5\n",
            "Epoch 11/100\n",
            "154764/154764 [==============================] - 322s 2ms/step - loss: 1.5115\n",
            "\n",
            "Epoch 00011: loss improved from 1.55604 to 1.51152, saving model to weights-improvement-11-1.5115.hdf5\n",
            "Epoch 12/100\n",
            "154764/154764 [==============================] - 325s 2ms/step - loss: 1.4733\n",
            "\n",
            "Epoch 00012: loss improved from 1.51152 to 1.47327, saving model to weights-improvement-12-1.4733.hdf5\n",
            "Epoch 13/100\n",
            "154764/154764 [==============================] - 323s 2ms/step - loss: 1.4377\n",
            "\n",
            "Epoch 00013: loss improved from 1.47327 to 1.43768, saving model to weights-improvement-13-1.4377.hdf5\n",
            "Epoch 14/100\n",
            "154764/154764 [==============================] - 324s 2ms/step - loss: 1.4047\n",
            "\n",
            "Epoch 00014: loss improved from 1.43768 to 1.40473, saving model to weights-improvement-14-1.4047.hdf5\n",
            "Epoch 15/100\n",
            "154764/154764 [==============================] - 320s 2ms/step - loss: 1.3731\n",
            "\n",
            "Epoch 00015: loss improved from 1.40473 to 1.37312, saving model to weights-improvement-15-1.3731.hdf5\n",
            "Epoch 16/100\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 1.3465\n",
            "\n",
            "Epoch 00016: loss improved from 1.37312 to 1.34652, saving model to weights-improvement-16-1.3465.hdf5\n",
            "Epoch 17/100\n",
            "154764/154764 [==============================] - 319s 2ms/step - loss: 1.3176\n",
            "\n",
            "Epoch 00017: loss improved from 1.34652 to 1.31759, saving model to weights-improvement-17-1.3176.hdf5\n",
            "Epoch 18/100\n",
            "154764/154764 [==============================] - 333s 2ms/step - loss: 1.2882\n",
            "\n",
            "Epoch 00018: loss improved from 1.31759 to 1.28824, saving model to weights-improvement-18-1.2882.hdf5\n",
            "Epoch 19/100\n",
            "154764/154764 [==============================] - 331s 2ms/step - loss: 1.2641\n",
            "\n",
            "Epoch 00019: loss improved from 1.28824 to 1.26410, saving model to weights-improvement-19-1.2641.hdf5\n",
            "Epoch 20/100\n",
            "154764/154764 [==============================] - 319s 2ms/step - loss: 1.2422\n",
            "\n",
            "Epoch 00020: loss improved from 1.26410 to 1.24224, saving model to weights-improvement-20-1.2422.hdf5\n",
            "Epoch 21/100\n",
            "154764/154764 [==============================] - 337s 2ms/step - loss: 1.2196\n",
            "\n",
            "Epoch 00021: loss improved from 1.24224 to 1.21959, saving model to weights-improvement-21-1.2196.hdf5\n",
            "Epoch 22/100\n",
            "154764/154764 [==============================] - 342s 2ms/step - loss: 1.1974\n",
            "\n",
            "Epoch 00022: loss improved from 1.21959 to 1.19738, saving model to weights-improvement-22-1.1974.hdf5\n",
            "Epoch 23/100\n",
            "154764/154764 [==============================] - 344s 2ms/step - loss: 1.1748\n",
            "\n",
            "Epoch 00023: loss improved from 1.19738 to 1.17481, saving model to weights-improvement-23-1.1748.hdf5\n",
            "Epoch 24/100\n",
            "154764/154764 [==============================] - 345s 2ms/step - loss: 1.1554\n",
            "\n",
            "Epoch 00024: loss improved from 1.17481 to 1.15535, saving model to weights-improvement-24-1.1554.hdf5\n",
            "Epoch 25/100\n",
            "154764/154764 [==============================] - 345s 2ms/step - loss: 1.1376\n",
            "\n",
            "Epoch 00025: loss improved from 1.15535 to 1.13758, saving model to weights-improvement-25-1.1376.hdf5\n",
            "Epoch 26/100\n",
            "154764/154764 [==============================] - 344s 2ms/step - loss: 1.1198\n",
            "\n",
            "Epoch 00026: loss improved from 1.13758 to 1.11980, saving model to weights-improvement-26-1.1198.hdf5\n",
            "Epoch 27/100\n",
            "154764/154764 [==============================] - 344s 2ms/step - loss: 1.1004\n",
            "\n",
            "Epoch 00027: loss improved from 1.11980 to 1.10041, saving model to weights-improvement-27-1.1004.hdf5\n",
            "Epoch 28/100\n",
            "154764/154764 [==============================] - 343s 2ms/step - loss: 1.0840\n",
            "\n",
            "Epoch 00028: loss improved from 1.10041 to 1.08397, saving model to weights-improvement-28-1.0840.hdf5\n",
            "Epoch 29/100\n",
            "154764/154764 [==============================] - 338s 2ms/step - loss: 1.0641\n",
            "\n",
            "Epoch 00029: loss improved from 1.08397 to 1.06411, saving model to weights-improvement-29-1.0641.hdf5\n",
            "Epoch 30/100\n",
            "154764/154764 [==============================] - 328s 2ms/step - loss: 1.0499\n",
            "\n",
            "Epoch 00030: loss improved from 1.06411 to 1.04992, saving model to weights-improvement-30-1.0499.hdf5\n",
            "Epoch 31/100\n",
            "154764/154764 [==============================] - 319s 2ms/step - loss: 1.0344\n",
            "\n",
            "Epoch 00031: loss improved from 1.04992 to 1.03436, saving model to weights-improvement-31-1.0344.hdf5\n",
            "Epoch 32/100\n",
            "154764/154764 [==============================] - 315s 2ms/step - loss: 1.0169\n",
            "\n",
            "Epoch 00032: loss improved from 1.03436 to 1.01693, saving model to weights-improvement-32-1.0169.hdf5\n",
            "Epoch 33/100\n",
            "154764/154764 [==============================] - 317s 2ms/step - loss: 1.0104\n",
            "\n",
            "Epoch 00033: loss improved from 1.01693 to 1.01039, saving model to weights-improvement-33-1.0104.hdf5\n",
            "Epoch 34/100\n",
            "154764/154764 [==============================] - 316s 2ms/step - loss: 0.9918\n",
            "\n",
            "Epoch 00034: loss improved from 1.01039 to 0.99181, saving model to weights-improvement-34-0.9918.hdf5\n",
            "Epoch 35/100\n",
            "154764/154764 [==============================] - 315s 2ms/step - loss: 0.9812\n",
            "\n",
            "Epoch 00035: loss improved from 0.99181 to 0.98116, saving model to weights-improvement-35-0.9812.hdf5\n",
            "Epoch 36/100\n",
            "154764/154764 [==============================] - 314s 2ms/step - loss: 0.9662\n",
            "\n",
            "Epoch 00036: loss improved from 0.98116 to 0.96618, saving model to weights-improvement-36-0.9662.hdf5\n",
            "Epoch 37/100\n",
            "154764/154764 [==============================] - 315s 2ms/step - loss: 0.9587\n",
            "\n",
            "Epoch 00037: loss improved from 0.96618 to 0.95871, saving model to weights-improvement-37-0.9587.hdf5\n",
            "Epoch 38/100\n",
            "154764/154764 [==============================] - 326s 2ms/step - loss: 0.9434\n",
            "\n",
            "Epoch 00038: loss improved from 0.95871 to 0.94340, saving model to weights-improvement-38-0.9434.hdf5\n",
            "Epoch 39/100\n",
            "154764/154764 [==============================] - 326s 2ms/step - loss: 0.9315\n",
            "\n",
            "Epoch 00039: loss improved from 0.94340 to 0.93154, saving model to weights-improvement-39-0.9315.hdf5\n",
            "Epoch 40/100\n",
            "154764/154764 [==============================] - 325s 2ms/step - loss: 0.9199\n",
            "\n",
            "Epoch 00040: loss improved from 0.93154 to 0.91986, saving model to weights-improvement-40-0.9199.hdf5\n",
            "Epoch 41/100\n",
            "154764/154764 [==============================] - 328s 2ms/step - loss: 0.9118\n",
            "\n",
            "Epoch 00041: loss improved from 0.91986 to 0.91180, saving model to weights-improvement-41-0.9118.hdf5\n",
            "Epoch 42/100\n",
            "154764/154764 [==============================] - 341s 2ms/step - loss: 0.9036\n",
            "\n",
            "Epoch 00042: loss improved from 0.91180 to 0.90359, saving model to weights-improvement-42-0.9036.hdf5\n",
            "Epoch 43/100\n",
            "154764/154764 [==============================] - 341s 2ms/step - loss: 0.8911\n",
            "\n",
            "Epoch 00043: loss improved from 0.90359 to 0.89113, saving model to weights-improvement-43-0.8911.hdf5\n",
            "Epoch 44/100\n",
            "154764/154764 [==============================] - 343s 2ms/step - loss: 0.8792\n",
            "\n",
            "Epoch 00044: loss improved from 0.89113 to 0.87923, saving model to weights-improvement-44-0.8792.hdf5\n",
            "Epoch 45/100\n",
            "154764/154764 [==============================] - 344s 2ms/step - loss: 0.8735\n",
            "\n",
            "Epoch 00045: loss improved from 0.87923 to 0.87349, saving model to weights-improvement-45-0.8735.hdf5\n",
            "Epoch 46/100\n",
            "154764/154764 [==============================] - 346s 2ms/step - loss: 0.8693\n",
            "\n",
            "Epoch 00046: loss improved from 0.87349 to 0.86929, saving model to weights-improvement-46-0.8693.hdf5\n",
            "Epoch 47/100\n",
            "154764/154764 [==============================] - 338s 2ms/step - loss: 0.8550\n",
            "\n",
            "Epoch 00047: loss improved from 0.86929 to 0.85501, saving model to weights-improvement-47-0.8550.hdf5\n",
            "Epoch 48/100\n",
            "154764/154764 [==============================] - 328s 2ms/step - loss: 0.8490\n",
            "\n",
            "Epoch 00048: loss improved from 0.85501 to 0.84903, saving model to weights-improvement-48-0.8490.hdf5\n",
            "Epoch 49/100\n",
            "154764/154764 [==============================] - 322s 2ms/step - loss: 1.5831\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.84903\n",
            "Epoch 50/100\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.6732\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.84903\n",
            "Epoch 51/100\n",
            " 81024/154764 [==============>...............] - ETA: 2:30 - loss: 2.5795Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W81fzbfLKSB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('./drive/My Drive/EIP3/Phase2/Session6-48-0.8490.hdf5.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJzdS_sKO5Z2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the network weights\n",
        "model.load_weights('./drive/My Drive/EIP3/Phase2/Session6-48-0.8490.hdf5.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwcR8cL_iy8q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "264cab0a-af69-41b0-a501-a32cf1287ad0"
      },
      "source": [
        "model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0722 05:05:13.247287 139985236694912 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "154764/154764 [==============================] - 324s 2ms/step - loss: 0.6823\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.68226, saving model to weights-improvement-01-0.6823.hdf5\n",
            "Epoch 2/50\n",
            "154764/154764 [==============================] - 319s 2ms/step - loss: 0.6664\n",
            "\n",
            "Epoch 00002: loss improved from 0.68226 to 0.66639, saving model to weights-improvement-02-0.6664.hdf5\n",
            "Epoch 3/50\n",
            "154764/154764 [==============================] - 319s 2ms/step - loss: 0.7600\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.66639\n",
            "Epoch 4/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 0.6709\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.66639\n",
            "Epoch 5/50\n",
            "154764/154764 [==============================] - 317s 2ms/step - loss: 0.6581\n",
            "\n",
            "Epoch 00005: loss improved from 0.66639 to 0.65813, saving model to weights-improvement-05-0.6581.hdf5\n",
            "Epoch 6/50\n",
            "154764/154764 [==============================] - 316s 2ms/step - loss: 0.6564\n",
            "\n",
            "Epoch 00006: loss improved from 0.65813 to 0.65642, saving model to weights-improvement-06-0.6564.hdf5\n",
            "Epoch 7/50\n",
            "154764/154764 [==============================] - 316s 2ms/step - loss: 0.6818\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.65642\n",
            "Epoch 8/50\n",
            "154764/154764 [==============================] - 316s 2ms/step - loss: 0.6413\n",
            "\n",
            "Epoch 00008: loss improved from 0.65642 to 0.64134, saving model to weights-improvement-08-0.6413.hdf5\n",
            "Epoch 9/50\n",
            "154764/154764 [==============================] - 316s 2ms/step - loss: 3.5897\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.64134\n",
            "Epoch 10/50\n",
            "154764/154764 [==============================] - 316s 2ms/step - loss: 3.0215\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.64134\n",
            "Epoch 11/50\n",
            "154764/154764 [==============================] - 317s 2ms/step - loss: 2.9367\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.64134\n",
            "Epoch 12/50\n",
            "154764/154764 [==============================] - 315s 2ms/step - loss: 2.8408\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.64134\n",
            "Epoch 13/50\n",
            "154764/154764 [==============================] - 315s 2ms/step - loss: 2.8007\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.64134\n",
            "Epoch 14/50\n",
            "154764/154764 [==============================] - 315s 2ms/step - loss: 2.8044\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.64134\n",
            "Epoch 15/50\n",
            "154764/154764 [==============================] - 315s 2ms/step - loss: 2.7598\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.64134\n",
            "Epoch 16/50\n",
            "154764/154764 [==============================] - 316s 2ms/step - loss: 2.7286\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.64134\n",
            "Epoch 17/50\n",
            "154764/154764 [==============================] - 316s 2ms/step - loss: 2.7097\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.64134\n",
            "Epoch 18/50\n",
            "154764/154764 [==============================] - 312s 2ms/step - loss: 2.6956\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.64134\n",
            "Epoch 19/50\n",
            "154764/154764 [==============================] - 313s 2ms/step - loss: 2.6832\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.64134\n",
            "Epoch 20/50\n",
            "154764/154764 [==============================] - 322s 2ms/step - loss: 2.6736\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.64134\n",
            "Epoch 21/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.6652\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.64134\n",
            "Epoch 22/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.6575\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.64134\n",
            "Epoch 23/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.6532\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.64134\n",
            "Epoch 24/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.6492\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.64134\n",
            "Epoch 25/50\n",
            "154764/154764 [==============================] - 317s 2ms/step - loss: 2.6485\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.64134\n",
            "Epoch 26/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.6467\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.64134\n",
            "Epoch 27/50\n",
            "154764/154764 [==============================] - 317s 2ms/step - loss: 2.6404\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.64134\n",
            "Epoch 28/50\n",
            "154764/154764 [==============================] - 319s 2ms/step - loss: 2.6360\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.64134\n",
            "Epoch 29/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.6299\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.64134\n",
            "Epoch 30/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.6237\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.64134\n",
            "Epoch 31/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.6199\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.64134\n",
            "Epoch 32/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.6132\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.64134\n",
            "Epoch 33/50\n",
            "154764/154764 [==============================] - 320s 2ms/step - loss: 2.6074\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.64134\n",
            "Epoch 34/50\n",
            "154764/154764 [==============================] - 319s 2ms/step - loss: 2.6015\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.64134\n",
            "Epoch 35/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.5977\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.64134\n",
            "Epoch 36/50\n",
            "154764/154764 [==============================] - 317s 2ms/step - loss: 2.5909\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.64134\n",
            "Epoch 37/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.5850\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.64134\n",
            "Epoch 38/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.5818\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.64134\n",
            "Epoch 39/50\n",
            "154764/154764 [==============================] - 317s 2ms/step - loss: 2.5776\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.64134\n",
            "Epoch 40/50\n",
            "154764/154764 [==============================] - 319s 2ms/step - loss: 2.5705\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.64134\n",
            "Epoch 41/50\n",
            "154764/154764 [==============================] - 319s 2ms/step - loss: 2.5657\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.64134\n",
            "Epoch 42/50\n",
            "154764/154764 [==============================] - 313s 2ms/step - loss: 2.5602\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.64134\n",
            "Epoch 43/50\n",
            "154764/154764 [==============================] - 313s 2ms/step - loss: 2.5561\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.64134\n",
            "Epoch 44/50\n",
            "154764/154764 [==============================] - 313s 2ms/step - loss: 2.5519\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.64134\n",
            "Epoch 45/50\n",
            "154764/154764 [==============================] - 313s 2ms/step - loss: 2.5480\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.64134\n",
            "Epoch 46/50\n",
            "154764/154764 [==============================] - 312s 2ms/step - loss: 2.5460\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.64134\n",
            "Epoch 47/50\n",
            "154764/154764 [==============================] - 313s 2ms/step - loss: 2.5392\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.64134\n",
            "Epoch 48/50\n",
            "154764/154764 [==============================] - 311s 2ms/step - loss: 2.5365\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.64134\n",
            "Epoch 49/50\n",
            "154764/154764 [==============================] - 311s 2ms/step - loss: 2.5328\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.64134\n",
            "Epoch 50/50\n",
            "154764/154764 [==============================] - 318s 2ms/step - loss: 2.5277\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.64134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f508ed364e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvEig2cBi9ah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('weights-improvement-08-0.6413.hdf5')\n",
        "model.save('./drive/My Drive/EIP3/Phase2/Session6-08-0.6413.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnTLIhSuAWOQ",
        "colab_type": "text"
      },
      "source": [
        "### Load Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZjVQd_UBtea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('./drive/My Drive/EIP3/Phase2/Session6-08-0.6413.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJrVQS-DAX6T",
        "colab_type": "text"
      },
      "source": [
        "### Int to char mappig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOGBNtAcL5cR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc_DwvWSAbB1",
        "colab_type": "text"
      },
      "source": [
        "### Predict with seed text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFM_yCJxEL5r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "b3a9a312-0ff9-4fb3-e003-dbaae61c38fb"
      },
      "source": [
        "import sys\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = list(dataX[start])\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" f\n",
            "the goldfish kept running in her head and she had a vague sort of idea\n",
            "that they must be collected \"\n",
            " at an inllithet than\n",
            "not loved \n",
            "the dormouse aglied she dorlouse wp another for you see a letter as she had fought in a loment she thought alice it would be offended and the orher thing alice was very uoleeiy she said to herself thats the bnok\n",
            "her pef yith her head at she spoke eat mast any of them with the dreatures was said alice and alice was soon for some minutes alice shouted in the dounousans and the moral of that isthe of the lobster quadrille that she had not at last the mock turtle sea\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}